{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import subprocess\n",
    "import dspy\n",
    "import logging\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from dspy.datasets import DataLoader\n",
    "from dspy.evaluate import Evaluate\n",
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch, LabeledFewShot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text to SQL with DSPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heavily pulled from this great notebook --> [DSPy-Text2SQL.ipynb](https://github.com/jjovalle99/DSPy-Text2SQL/blob/23a0a347db2d7515c5a28c305dacaea00d09dddc/DSPy-Text2SQL.ipynb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "########## DSPy Config  ##########\n",
    "lm = dspy.OllamaLocal(\"open-hermes-2-4_0\", max_tokens=2000, model_type=\"text\")\n",
    "evaluator_lm = dspy.OllamaLocal(\"nous-llama-3-8b-instruct\", max_tokens=4000, model_type=\"text\")\n",
    "\n",
    "dspy.settings.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing inference of `open-hermes` and the quantized `nous-llama-3-instruct`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm(\"Who is Bodhi in Point Break?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_lm(prompt=\"Who is Johnny Utah?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Arize Phoenix Telemetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Arize Phoenix ##########\n",
    "import phoenix as px\n",
    "from openinference.instrumentation.dspy import DSPyInstrumentor\n",
    "from opentelemetry import trace as trace_api\n",
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
    "from opentelemetry.sdk import trace as trace_sdk\n",
    "from opentelemetry.sdk.resources import Resource\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
    "from openinference.semconv.trace import SpanAttributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoenix_session = px.launch_app()\n",
    "endpoint = \"http://localhost:6006/v1/traces\"\n",
    "resource = Resource(attributes={})\n",
    "tracer_provider = trace_sdk.TracerProvider(resource=resource)\n",
    "span_otlp_exporter = OTLPSpanExporter(endpoint=endpoint)\n",
    "tracer_provider.add_span_processor(SimpleSpanProcessor(span_exporter=span_otlp_exporter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_api.set_tracer_provider(tracer_provider=tracer_provider)\n",
    "DSPyInstrumentor().instrument()\n",
    "class TestTrace(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Perform the task requested to the best of your ability.\n",
    "    \"\"\"\n",
    "\n",
    "    task = dspy.InputField(desc=\"Task to be performed.\")\n",
    "    answer = dspy.OutputField(desc=\"The answer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracer = trace_api.get_tracer(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_trace = dspy.Predict(TestTrace)\n",
    "answer = test_trace(task=\"Say Hello.\")\n",
    "test_trace_cot = dspy.ChainOfThought(TestTrace)\n",
    "pred = test_trace_cot(task=\"What would Bodhi from Point Break say is the key to surfing?\")\n",
    "pred.answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using [Synthetic Text to Sql](https://huggingface.co/datasets/gretelai/synthetic_text_to_sql/viewer/default/train?row=1) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader()\n",
    "dataset = dl.from_huggingface(\n",
    "    dataset_name=\"gretelai/synthetic_text_to_sql\", # Dataset name from Huggingface\n",
    "    fields=(\"sql_prompt\", \"sql_context\", \"sql\"), # Fields needed\n",
    "    input_keys=(\"sql_prompt\", \"sql_context\") # What our model expects to recieve to generate an output\n",
    ")\n",
    "trainset, testset = dl.sample(dataset[\"train\"], n=50), dl.sample(dataset[\"test\"], n=25) # 50 training samples, 25 testing samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify an example of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dl.sample(dataset=trainset, n=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in sample.items():\n",
    "    print(f\"\\n{k.upper()}:\\n\")\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Signature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextToSql(dspy.Signature):\n",
    "    \"\"\"Transform a natural language query into a SQL query.\"\"\"\n",
    "\n",
    "    sql_prompt = dspy.InputField(desc=\"Natural language query\")\n",
    "    sql_context = dspy.InputField(desc=\"Context for the query\")\n",
    "    sql = dspy.OutputField(desc=\"SQL query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sql_query = dspy.Predict(signature=TextToSql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = generate_sql_query(sql_prompt=sample[\"sql_prompt\"], sql_context=sample[\"sql_context\"])\n",
    "\n",
    "for k, v in result.items():\n",
    "    print(f\"\\n{k.upper()}:\\n\")\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric of Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Correctness(dspy.Signature):\n",
    "    \"\"\"Assess if the SQL query accurately answers the given natural language query based on the provided context.\"\"\"\n",
    "\n",
    "    sql_prompt = dspy.InputField(desc=\"Natural language query\")\n",
    "    sql_context = dspy.InputField(desc=\"Context for the query\")\n",
    "    sql = dspy.InputField(desc=\"SQL query\")\n",
    "    correct = dspy.OutputField(desc=\"Indicate whether the SQL query correctly answers the natural language query based on the given context\", prefix=\"Yes/No:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correctness_metric(example, pred, trace=None):\n",
    "    sql_prompt, sql_context, sql = example.sql_prompt, example.sql_context, pred.sql\n",
    "\n",
    "    # current_span = trace_api.get_current_span()\n",
    "    # current_span.set_attribute(SpanAttributes.LLM_MODEL_NAME, \"llama-3\")\n",
    "\n",
    "    correctness = dspy.Predict(Correctness)\n",
    "\n",
    "    with dspy.context(lm=evaluator_lm):\n",
    "        correct = correctness(\n",
    "            sql_prompt=sql_prompt,\n",
    "            sql_context=sql_context,\n",
    "            sql=sql,\n",
    "        )\n",
    "\n",
    "    score = int(correct.correct == \"Yes\")\n",
    "\n",
    "    if trace is not None:\n",
    "        return score == 1\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Single Data Point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_correctness = correctness_metric(example=sample, pred=result)\n",
    "print(f\"Correct SQL query: {'Yes' if _correctness else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To-do: Set up Metadata Tracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tracer.start_as_current_span(\"testing metadata\") as parent:\n",
    "#     current_span = trace_api.get_current_span()\n",
    "#     current_span.set_attribute(\"model\", \"llama-3-7b-instruct-5q\")\n",
    "#     evaluator_lm(\"Hello there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class QuestionClassifier(dspy.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         # [snip]\n",
    "#     def forward(self, question: str) -> tuple[str,str]:\n",
    "#         current_span = trace_api.get_current_span()\n",
    "#         current_span.set_attribute(\"metadata\", \"{ 'foo': 'bar' }\")\n",
    "#         # [your dspy code]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Entire Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open Hermes 2(quantized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantized `open-hermes` outperforms `gpt-3.5-turbo-0125`, with `gpt` scoring about 68% and `open-hermes` scoring about 80% OOTB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with dspy.context(lm=lm):\n",
    "    evaluate = Evaluate(\n",
    "        devset=testset,\n",
    "        metric=correctness_metric,\n",
    "        num_threads=8,\n",
    "        display_progress=True,\n",
    "        display_table=5,\n",
    "    )\n",
    "    evaluate(generate_sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Llama 3 7b (quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with dspy.context(lm=evaluator_lm):\n",
    "    evaluate = Evaluate(\n",
    "        devset=testset,\n",
    "        metric=correctness_metric,\n",
    "        num_threads=8,\n",
    "        display_progress=True,\n",
    "        display_table=5,\n",
    "    )\n",
    "    evaluate(generate_sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize for Text-to-SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextToSqlProgram(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.program = dspy.ChainOfThought(signature=TextToSql)\n",
    "\n",
    "    def forward(self, sql_prompt, sql_context):\n",
    "        return self.program(sql_prompt=sql_prompt, sql_context=sql_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute optimizer -> This only adds a few shots to the prompt\n",
    "optimizer = LabeledFewShot(k=4)\n",
    "optimized_program = optimizer.compile(student=TextToSqlProgram(), trainset=trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_program(sql_context=sample[\"sql_context\"], sql_prompt=sample[\"sql_prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Optimized Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(optimized_program, num_threads=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BootstrapFewShotWithRandomSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Rules for DSPy(at the moment):\n",
    "- If you have few examples(~10) use `BootstrapFewShot`\n",
    "- If you have more(~50) use `BootstrapFewShotWithRandomSearch`\n",
    "- If you have a lot(~300) use `MIPRO`\n",
    "- If you have been able to use one of these with a large LM (e.g., 7B\n",
    "parameters or above) and need a very efficient program, compile that\n",
    "down to a small LM with `BootstrapFinetune`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer2 = BootstrapFewShotWithRandomSearch(\n",
    "    metric=correctness_metric, max_bootstrapped_demos=2, num_candidate_programs=8, num_threads=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_program_2 = optimizer2.compile(\n",
    "    student=TextToSqlProgram(), trainset=trainset, valset=testset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
